{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ff867b",
   "metadata": {},
   "source": [
    "# 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63678a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4910d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "print(hs.shape)\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf76de4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec7b1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * dt\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09b7094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd20f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.np import *\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "037781b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66c0c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7368b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import *\n",
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3175931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3715289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3eca8edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 14[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 31[s] | 손실 1.89\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 45[s] | 손실 1.73\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 59[s] | 손실 1.49\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 74[s] | 손실 1.23\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 91[s] | 손실 1.15\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 106[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 122[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 138[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 154[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 168[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 182[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 196[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 210[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 225[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 240[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 254[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1999-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1999-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1999-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1999-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1999-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1999-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1999-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1999-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1999-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1999-08-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 14[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 27[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 41[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 55[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 68[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 82[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 96[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 109[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 123[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 137[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 151[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 165[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 179[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 192[s] | 손실 0.97\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 206[s] | 손실 0.96\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 220[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 234[s] | 손실 0.95\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1993-05-20\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1991-05-15\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2002-06-22\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1993-05-15\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1991-01-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1992-05-20\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 2002-06-20\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2009-08-20\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1992-05-20\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1993-05-15\n",
      "---\n",
      "val acc 0.040%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 14[s] | 손실 0.93\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 28[s] | 손실 0.93\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 42[s] | 손실 0.92\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 56[s] | 손실 0.90\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 69[s] | 손실 0.89\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 82[s] | 손실 0.88\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 95[s] | 손실 0.86\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 109[s] | 손실 0.85\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 122[s] | 손실 0.84\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 135[s] | 손실 0.83\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 149[s] | 손실 0.83\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 163[s] | 손실 0.81\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 177[s] | 손실 0.81\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 191[s] | 손실 0.79\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 205[s] | 손실 0.75\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 219[s] | 손실 0.74\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 232[s] | 손실 0.71\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 2004-11-04\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1982-10-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2002-03-26\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 2012-11-21\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1977-09-01\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 2003-11-13\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1980-09-25\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1972-09-03\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1990-10-25\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1982-11-12\n",
      "---\n",
      "val acc 1.000%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 14[s] | 손실 0.69\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 28[s] | 손실 0.67\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 41[s] | 손실 0.65\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 55[s] | 손실 0.63\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 68[s] | 손실 0.62\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 82[s] | 손실 0.61\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 96[s] | 손실 0.59\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 112[s] | 손실 0.56\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 127[s] | 손실 0.54\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 142[s] | 손실 0.54\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 157[s] | 손실 0.52\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 172[s] | 손실 0.50\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 187[s] | 손실 0.48\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 203[s] | 손실 0.48\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 217[s] | 손실 0.44\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 231[s] | 손실 0.43\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 246[s] | 손실 0.42\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1994-10-09\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2012-10-17\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2002-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 2012-11-27\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1974-06-14\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1993-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1985-08-20\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1970-08-26\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1993-10-20\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2011-01-15\n",
      "---\n",
      "val acc 13.540%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 15[s] | 손실 0.40\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 31[s] | 손실 0.38\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 46[s] | 손실 0.36\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 61[s] | 손실 0.35\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 77[s] | 손실 0.33\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 91[s] | 손실 0.33\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 106[s] | 손실 0.32\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 120[s] | 손실 0.30\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 134[s] | 손실 0.28\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 147[s] | 손실 0.26\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 160[s] | 손실 0.26\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 174[s] | 손실 0.25\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 187[s] | 손실 0.25\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 201[s] | 손실 0.23\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 215[s] | 손실 0.21\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 229[s] | 손실 0.20\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 242[s] | 손실 0.18\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2001-11-10\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2003-05-22\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1990-10-09\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2006-08-27\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-01-16\n",
      "---\n",
      "val acc 45.760%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 14[s] | 손실 0.17\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 28[s] | 손실 0.17\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 42[s] | 손실 0.16\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 55[s] | 손실 0.15\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 68[s] | 손실 0.14\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 81[s] | 손실 0.13\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 94[s] | 손실 0.13\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 107[s] | 손실 0.13\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 120[s] | 손실 0.11\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 134[s] | 손실 0.10\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 147[s] | 손실 0.10\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 160[s] | 손실 0.10\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 173[s] | 손실 0.11\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 186[s] | 손실 0.10\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 199[s] | 손실 0.08\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 212[s] | 손실 0.07\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 225[s] | 손실 0.06\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1995-10-14\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 87.600%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.05\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 13[s] | 손실 0.05\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 26[s] | 손실 0.05\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 39[s] | 손실 0.04\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 53[s] | 손실 0.04\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 66[s] | 손실 0.05\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 79[s] | 손실 0.04\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 92[s] | 손실 0.03\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 105[s] | 손실 0.03\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 119[s] | 손실 0.03\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 133[s] | 손실 0.03\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 147[s] | 손실 0.04\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 161[s] | 손실 0.04\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 176[s] | 손실 0.03\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 190[s] | 손실 0.03\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 205[s] | 손실 0.03\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 218[s] | 손실 0.04\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 232[s] | 손실 0.04\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1995-10-14\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 95.280%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.02\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 14[s] | 손실 0.02\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 27[s] | 손실 0.02\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 40[s] | 손실 0.02\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 53[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 66[s] | 손실 0.02\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 79[s] | 손실 0.02\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 95[s] | 손실 0.02\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 111[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 124[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 137[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 150[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 164[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 177[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 190[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 204[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 217[s] | 손실 0.01\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 230[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 98.520%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 14[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 28[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 42[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 56[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 69[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 83[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 97[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 111[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 124[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 137[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 151[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 164[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 177[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 190[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 203[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 216[s] | 손실 0.01\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 229[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 98.900%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 14[s] | 손실 0.01\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 27[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 40[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 92[s] | 손실 0.01\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 105[s] | 손실 0.04\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 118[s] | 손실 0.02\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 131[s] | 손실 0.02\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 144[s] | 손실 0.01\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 156[s] | 손실 0.01\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 169[s] | 손실 0.01\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 182[s] | 손실 0.01\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 195[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 208[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 221[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.940%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../ch07')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "               batch_size=batch_size, max_grad=max_grad)\n",
    "    \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                   id_to_char, verbose, is_reverse=True)\n",
    "        \n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "    \n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd6d865c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
